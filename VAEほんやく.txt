Auto-Encoding Variational Bayes



Abstract




How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? 

We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. 
難解な事後分布を持つ連続的な潜在変数や大規模データセットの存在下で、有向確率モデルにおける効率的な推論と学習を行うにはどうすればよいか？
我々は、大規模なデータセットにスケールし、いくつかの軽度な微分可能性条件の下で、難解なケースでも動作する確率的変分推論と学習アルゴリズムを紹介する。

Our contributions is two-fold. 
我々の貢献は2つある。

First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. 
第一に、我々は、変分的下界の再パラメータ化により、標準的な確率的勾配法を用いて簡単に最適化できる下界推定量が得られることを示す。

Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator.
第二に、データポイントごとに連続的な潜在変数を持つI.I.D.データセットの場合、提案された下位境界推定量を用いて、難解な事後処理に近似推論モデル（認識モデルとも呼ばれる）を適合させることで、事後推論が特に効率的になることを示す。

Theoretical advantages are reflected in experimental results.
理論的な利点は実験結果に反映されている。




Introduction



How can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions? 

The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. 

Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. 

We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.


For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-Encoding VB (AEVB) algorithm. 

In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. 

The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. 

When a neural network is used for the recognition model, we arrive at the variational auto-encoder.






Method

The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. 

We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example,straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. 

Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity.


Problem scenario


Let us consider some dataset X = {x(i)}N i=1 consisting of N i.i.d. samples of some continuous or discrete variable x. We assume that the data are generated by some random process, involving an unobserved continuous random variable z. 

The process consists of two steps: (1) a value z(i) is generated from some prior distribution pθ∗(z); (2) a value x(i) is generated from some conditional distribution pθ∗(x|z). 

We assume that the prior pθ∗(z) and likelihood pθ∗(x|z) come from parametric families of distributions pθ(z) and pθ(x|z), and that their PDFs are differentiable almost everywhere w.r.t. both θ and z. 

Unfortunately, a lot of this process is hidden from our view: the true parameters θ∗ as well as the values of the latent variables z(i) are unknown to us.

Very importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. 

Conversely, we are here interested in a general algorithm that even works efficiently in the case of



1. Intractability:

the case where the integral of the marginal likelihood pθ(x)=�pθ(z)pθ(x|z) dz is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density pθ(z|x) = pθ(x|z)pθ(z)/pθ(x) is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-field VB algorithm are also intractable. 

These intractabilities are quite common and appear in cases of moderately complicated likelihood functions pθ(x|z), e.g. a neural network with a nonlinear hidden layer.

2. A large dataset: we have so much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. 

Sampling-based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint.













We are interested in, and propose a solution to, three related problems in the above scenario:

1. Efficient approximate ML or MAP estimation for the parameters θ. 

The parameters can be of interest themselves, e.g. if we are analyzing some natural process. 

They also allow us to mimic the hidden random process and generate artificial data that resembles the real data.


2. Efficient approximate posterior inference of the latent variable z given an observed value x for a choice of parameters θ. This is useful for coding or data representation tasks.


3. Efficient approximate marginal inference of the variable x. 

This allows us to perform all kinds of inference tasks where a prior over x is required. Common applications in computer vision include image denoising, inpainting and super-resolution.










For the purpose of solving the above problems, let us introduce a recognition model qφ(z|x): an approximation to the intractable true posterior pθ(z|x). 

Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters φ are not computed from some closed-form expectation. 

Instead, we’ll introduce a method for learning the recognition model parameters φ jointly with the generative model parameters θ.


From a coding theory perspective, the unobserved variables z have an interpretation as a latent representation or code. 

In this paper we will therefore also refer to the recognition model qφ(z|x) as a probabilistic encoder, since given a datapoint x it produces a distribution (e.g. a Gaussian) over the possible values of the code z from which the datapoint x could have been generated. 

In a similar vein we will refer to pθ(x|z) as a probabilistic decoder, since given a code z it produces a distribution over the possible corresponding values of x.









The variational bound



The SGVB estimator and AEVB algorithm









Often, the KL-divergence DKL(qφ(z|x(i))||pθ(z)) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error Eqφ(z|x(i)) � log pθ(x(i)|z) � requires estimation by sampling. 

The KL-divergence term can then be interpreted as regularizing φ, encouraging the approximate posterior to be close to the prior pθ(z). 

This yields a second version of the SGVB estimator �LB(θ, φ; x(i)) ≃ L(θ, φ; x(i)), corresponding to eq. (3), which typically has less variance than the generic estimator:

�LB(θ, φ; x(i)) = −DKL(qφ(z|x(i))||pθ(z)) + 1 L L � l=1 (log pθ(x(i)|z(i,l))) where z(i,l) = gφ(ϵ(i,l), x(i)) and ϵ(l) ∼ p(ϵ) (7)

Given multiple datapoints from a dataset X with N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:
L(θ, φ; X) ≃ �LM(θ, φ; XM) = N M M � i=1 �L(θ, φ; x(i)) (8)

where the minibatch XM = {x(i)}M i=1 is a randomly drawn sample of M datapoints from the full dataset X with N datapoints. 

In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M = 100.


Derivatives ∇θ,φ �L(θ; XM) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. 

See algorithm 1 for a basic approach to compute the stochastic gradients.


A connection with auto-encoders becomes clear when looking at the objective function given at eq. (7). 

The first term is (the KL divergence of the approximate posterior from the prior) acts as a regularizer, while the second term is a an expected negative reconstruction error. 

The function gφ(.) is chosen such that it maps a datapoint x(i) and a random noise vector ϵ(l) to a sample from the approximate posterior for that datapoint: z(i,l) = gφ(ϵ(l), x(i)) where z(i,l) ∼ qφ(z|x(i)). 


Subsequently, the sample z(i,l) is then input to function log pθ(x(i)|z(i,l)), which equals the probability density (or mass) of datapoint x(i) under the generative model, given z(i,l). 

This term is a negative reconstruction error in auto-encoder parlance.




















4
Related work


The wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learning method in the literature that is applicable to the same general class of continuous latent variable models. 

Like our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. 

A drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood. 

An advantage of wake-sleep is that it also applies to models with discrete latent variables. 

Wake-Sleep has the same computational complexity as AEVB per datapoint.


Stochastic variational inference [HBWP13] has recently received increasing interest.


Recently, [BJP12] introduced a control variate schemes to reduce the high variance of the na¨ıve gradient estimator discussed in section 2.1, and applied to exponential family approximations of the posterior. 

In [RGB13] some general methods, i.e. a control variate scheme, were introduced for reducing the variance of the original gradient estimator. 

In [SK13], a similar reparameterization as in this paper was used in an efficient version of a stochastic variational inference algorithm for learning the natural parameters of exponential-family approximating distributions.


The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. 

A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known. 

In [Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(z) = N(0, I) and a conditional distribution p(x|z) = N(x; Wz, ϵI), specifically the case with infinitesimally small ϵ.


In relevant recent work on autoencoders [VLL+10] it was shown that the training criterion of unregularized autoencoders corresponds to maximization of a lower bound (see the infomax principle [Lin89]) of the mutual information between input X and latent representation Z. 

Maximizing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional entropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding model [VLL+10], i.e. the negative reconstrution error. 

However, it is well known that this reconstruction criterion is in itself not sufficient for learning useful representations [BCV13]. 

Regularization techniques have been proposed to make autoencoders learn useful representations, such as denoising, contractive and sparse autoencoder variants [BCV13]. 

The SGVB objective contains a regularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regularization hyperparameter required to learn useful representations. 

Related are also encoder-decoder architectures such as the predictive sparse decomposition (PSD) [KRL08], from which we drew some inspiration. 

Also relevant are the recently introduced Generative Stochastic Networks [BTL13] where noisy auto-encoders learn the transition operator of a Markov chain that samples from the data distribution. 

In [SL10] a recognition model was employed for efficient learning with Deep Boltzmann Machines. 

These methods are targeted at either unnormalized models (i.e. undirected models like Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm for learning a general class of directed probabilistic models.

The recently proposed DARN method [GMW13], also learns a directed probabilistic model using an auto-encoding structure, however their method applies to binary latent variables. 

Even more recently, [RMW14] also make the connection between auto-encoders, directed proabilistic models and stochastic variational inference using the reparameterization trick we describe in this paper.

Their work was developed independently of ours and provides an additional perspective on AEVB.






5
Experiments

We trained generative models of images from the MNIST and Frey Face datasets3 and compared learning algorithms in terms of the variational lower bound, and the estimated marginal likelihood.

The generative model (encoder) and variational approximation (decoder) from section 3 were used, where the described encoder and decoder have an equal number of hidden units. 

Since the Frey Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except that the means were constrained to the interval (0, 1) using a sigmoidal activation function at the decoder output. 

Note that with hidden units we refer to the hidden layer of the neural networks of the encoder and decoder.


Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator ∇θ,φL(θ, φ; X) (see algorithm 1), plus a small weight decay term corresponding to a prior p(θ) = N(0, I). 

Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound.

We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-encoder. 

All parameters, both variational and generative, were initialized by random sampling from N(0, 0.01), and were jointly stochastically optimized using the MAP criterion. 

Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01,0.02, 0.1} based on performance on the training set in the first few iterations. 

Minibatches of size M = 100 were used, with L = 1 samples per datapoint.


Likelihood lower bound We trained generative models (decoders) and corresponding encoders (a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case of the Frey Face dataset (to prevent overfitting, since it is a considerably smaller dataset). 

The chosen number of hidden units is based on prior literature on auto-encoders, and the relative performance of different algorithms was not very sensitive to these choices. 

Figure 2 shows the results when comparing the lower bounds. 

Interestingly, superfluous latent variables did not result in overfitting, which is explained by the regularizing nature of the variational bound.

Marginal likelihood For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. 

More information about the marginal likelihood estimator is available in the appendix. 

For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. 

Again, the MNIST dataset was used. The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo(HMC) [DKPR87] sampler; details are in the appendix. 

We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure 3.






Visualisation of high-dimensional data If we choose a low-dimensional latent space (e.g. 2D), we can use the learned encoders (recognition model) to project high-dimensional data to a low-dimensional manifold. See appendix A for visualisations of the 2D latent manifolds for the MNIST and Frey Face datasets.




6
Conclusion

We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables. 

The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. 

For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. 

The theoretical advantages are reflected in experimental results





Future work

Since the SGVB estimator and the AEVB algorithm can be applied to almost any inference and learning problem with continuous latent variables, there are plenty of future directions: (i) learning hierarchical generative architectures with deep neural networks (e.g. convolutional networks) used for the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic Bayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models with latent variables, useful for learning complicated noise distributions.





